{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='optimization-solver-packages'></a>\n",
    "How to read this lecture…\n",
    "\n",
    "- If using QuantEcon lectures for the first time on a computer, execute ] add InstantiateFromURL inside of a notebook or the REPL  \n",
    "- For some notebooks, enable content with “Trust” on the command tab of Jupyter lab  \n",
    "- Code should execute sequentially if run in a Jupyter notebook  \n",
    "- Please direct feedback to [contact@quantecon.org](mailto:contact@quantecon.org\") or [discourse forum](http://discourse.quantecon.org/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solvers, Optimizers, and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Solvers, Optimizers, and Automatic Differentiation](#Solvers,-Optimizers,-and-Automatic-Differentiation)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Introduction to Automatic Differentiation](#Introduction-to-Automatic-Differentiation)  \n",
    "  - [Optimization](#Optimization)  \n",
    "  - [Systems of Equations and Least Squares](#Systems-of-Equations-and-Least-Squares)  \n",
    "  - [LeastSquaresOptim.jl](#LeastSquaresOptim.jl)  \n",
    "  - [Exercises](#Exercises)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lecture we introduce a few of the Julia libraries that we’ve found particularly useful for quantitative work in economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using InstantiateFromURL\n",
    "activate_github(\"QuantEcon/QuantEconLectureAllPackages\", tag = \"v0.9.0\") # activate the QuantEcon environment\n",
    "\n",
    "using LinearAlgebra, Statistics, Compat # load common packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Automatic Differentiation\n",
    "\n",
    "Automatic differentiation (sometimes called algorithmic differentiation) is a crucial way to increase the performance of both estimation and solution methods\n",
    "\n",
    "There are essentially three four ways to calculate the gradient or jacobian on a computer\n",
    "\n",
    "- Calculation by hand  \n",
    "  \n",
    "      Where possible, you can calculate the derivative on “pen-and-paper” and potentially simplify the expression\n",
    "  \n",
    "  Sometimes, though not always, the most accurate and fastest option if there are algebraic simplifications\n",
    "  \n",
    "  The algebra is error prone for non-trivial setups-   \n",
    "  -   \n",
    "  -   \n",
    "- Finite differences  \n",
    "  \n",
    "      Evaluate the function at least N times to get the gradient, jacobians are even worse\n",
    "  \n",
    "  Large  is numerically stable but inaccurate, too small of  is numerically unstable but more accurate\n",
    "  \n",
    "  Avoid if you can, and use packages (e.g. DiffEqDiffTools.jl ) to get a good choice of -   \n",
    "  - $ \\Delta $$ \\Delta $  \n",
    "  - [](https://github.com/JuliaDiffEq/DiffEqDiffTools.jl)$ \\Delta $  \n",
    "\n",
    "\n",
    "$$\n",
    "\\partial_{x_i}f(x_1,\\ldots x_N) \\approx \\frac{f(x_1,\\ldots x_i + \\Delta,\\ldots x_N) - f(x_1,\\ldots x_i,\\ldots x_N)}{\\Delta}\n",
    "$$\n",
    "\n",
    "- Symbolic differentiation  \n",
    "  \n",
    "      If you put in an expression for a function, some packages will do symbolic differentiation\n",
    "  \n",
    "  In effect, repeated applications of the chain-rule, produce-rule, etc.\n",
    "  \n",
    "  Sometimes a good solution, if the package can handle your functions-   \n",
    "  -   \n",
    "  -   \n",
    "- Automatic Differentiation  \n",
    "  \n",
    "      Essentially the same as symbolic differentiation, just occurring at a different time in the compilation process\n",
    "  \n",
    "  Equivalent to analytical derivatives since it uses the chain-rule, etc.-   \n",
    "  -   \n",
    "\n",
    "\n",
    "We will explore AD packages in Julia rather than the alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "To summarize here, first recall the chain rule (adapted from Wikipedia)\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{dw} \\frac{dw}{dx}\n",
    "$$\n",
    "\n",
    "Consider functions composed of calculations with fundamental operations with known analytical derivatives, such as $ f(x_1, x_2) = x_1 x_2 + \\sin(x_1) $\n",
    "\n",
    "To compute $ \\frac{d f(x_1,x_2)}{d x_1} $\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|l}\n",
    "\\text{Operations to compute value} &\n",
    "\\text{Operations to compute $\\frac{d f(x_1,x_2)}{d x_1}$}\n",
    "\\\\\n",
    "\\hline\n",
    "w_1 = x_1 &\n",
    "\\frac{d w_1}{d x_1} = 1 \\text{ (seed)}\\\\\n",
    "w_2 = x_2 &\n",
    "\\frac{d  w_2}{d x_1} = 0 \\text{ (seed)}\n",
    "\\\\\n",
    "w_3 = w_1 \\cdot w_2 &\n",
    "\\frac{d  w_3}{d x_1} = w_2 \\cdot \\frac{d  w_1}{d x_1} + w_1 \\cdot \\frac{d  w_2}{d x_1}\n",
    "\\\\\n",
    "w_4 = \\sin w_1 &\n",
    "\\frac{d  w_4}{d x_1} = \\cos w_1 \\cdot \\frac{d  w_1}{d x_1}\n",
    "\\\\\n",
    "w_5 = w_3 + w_4 &\n",
    "\\frac{d  w_5}{d x_1} = \\frac{d  w_3}{d x_1} + \\frac{d  w_4}{d x_1}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dual Numbers\n",
    "\n",
    "One way to implement this (used in Forward-mode AD) is to use [Dual Numbers](https://en.wikipedia.org/wiki/Dual_number)\n",
    "\n",
    "Take a number $ x $ and augment it with an infinitesimal $ \\epsilon $ such that $ \\epsilon^2 = 0 $, i.e. $ x \\to x + x' \\epsilon $\n",
    "\n",
    "All math is then done with this (mathematical, rather than Julia) tuple $ (x, x') $ where the $ x' $ may be hidden from the user\n",
    "\n",
    "With these definition, we can write a general rule for differentiation of $ g(x,y) $ as\n",
    "\n",
    "$$\n",
    "g(\\left(x,x'\\right),\\left(y,y'\\right)) = \\left(g(x,y),\\partial_x g(x,y)x' + \\partial_y g(x,y)y' \\right)\n",
    "$$\n",
    "\n",
    "This calculation is simply the chain rule for the total derivative\n",
    "\n",
    "An AD library using dual numbers will concurrently calculate the function and its derivatives, repeating the chain rule until it hits a set of intrinsic rules such as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x + y \\to \\left(x,x'\\right) + \\left(y,y'\\right) &= \\left(x + y,\\underbrace{x' + y'}_{\\partial(x + y) = \\partial x + \\partial y}\\right)\\\\\n",
    "x y \\to \\left(x,x'\\right) \\times \\left(y,y'\\right) &= \\left(x y,\\underbrace{x'y + y'x}_{\\partial(x y) = y \\partial x + x \\partial y y}\\right)\\\\\n",
    "\\exp(x) \\to \\exp(\\left(x, x'\\right)) &= \\left(\\exp(x),\\underbrace{x'\\exp(x)}_{\\partial(\\exp(x)) = \\exp(x)\\partial x} \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ForwardDiff.jl\n",
    "\n",
    "We have already seen one of the AD packages in Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/arnavsood/.julia/compiled/v1.0/ForwardDiff/k0ETY.ji for ForwardDiff [f6369f11-7733-5829-9624-2563aa707210]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForwardDiff.gradient(h, x) = [26.3548 16.6631]\n",
      "g(rand(20)) = [0.93898, 0.893705, 0.917452, 0.87998, 1.0, 0.996822, 0.5439, 0.734483, 0.896878, 0.898396, 0.704589, 0.931328, 0.897029, 0.967132, 0.981794, 0.841244, 0.916563, 0.982085, 0.85773, 0.999822]\n"
     ]
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "h(x) = sin(x[1]) + x[1] * x[2] + sinh(x[1] * x[2]) # multivariate.\n",
    "x = [1.4 2.2]\n",
    "@show ForwardDiff.gradient(h,x) # use AD, seeds from x\n",
    "\n",
    "#Or, can use complicated functions of many variables\n",
    "f(x) = sum(sin, x) + prod(tan, x) * sum(sqrt, x)\n",
    "g = (x) -> ForwardDiff.gradient(f, x); # g() is now the gradient\n",
    "@show g(rand(20)); # gradient at a random point\n",
    "# ForwardDiff.hessian(f,x2) # or the hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even auto-differenitate complicated functions with embedded iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function squareroot(x) #pretending we don't know sqrt()\n",
    "    z = copy(x) # Initial starting point for Newton’s method\n",
    "    while abs(z*z - x) > 1e-13\n",
    "        z = z - (z*z-x)/(2z)\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "sqrt(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35355339059327373"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "dsqrt(x) = ForwardDiff.derivative(squareroot, x)\n",
    "dsqrt(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flux.jl\n",
    "\n",
    "Another is Flux.jl, which is a machine-learning library in Julia\n",
    "\n",
    "AD is one of the main reasons that machine learning has become so powerful in recent years, and is an essential component of any ML package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/arnavsood/.julia/compiled/v1.0/Flux/QdkVy.ji for Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.0 (tracked)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux.Tracker\n",
    "using Flux.Tracker: update!\n",
    "\n",
    "f(x) = 3x^2 + 2x + 1\n",
    "\n",
    "# df/dx = 6x + 2\n",
    "df(x) = Tracker.gradient(f, x)[1]\n",
    "\n",
    "df(2) # 14.0 (tracked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Adjoint{Float64,Array{Float64,2}}:\n",
       " 0.697753  0.879331 \n",
       " 0.828623  0.0438626"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(2,2)\n",
    "f(x) = A * x\n",
    "x0 = [0.1, 2.0]\n",
    "f(x0)\n",
    "Flux.jacobian(f, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can differentiate complicated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsquareroot (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsquareroot(x) = Tracker.gradient(squareroot, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation, we can do a machine-learning approach to a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.449798213832934"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = rand(2, 5)\n",
    "b = rand(2)\n",
    "\n",
    "predict(x) = W*x .+ b\n",
    "\n",
    "function loss(x, y)\n",
    "ŷ = predict(x)\n",
    "sum((y .- ŷ).^2)\n",
    "end\n",
    "\n",
    "x, y = rand(5), rand(2) # Dummy data\n",
    "loss(x, y) # ~ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0301487987564637 (tracked)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = param(W)\n",
    "b = param(b)\n",
    "\n",
    "gs = Tracker.gradient(() -> loss(x, y), Params([W, b]))\n",
    "\n",
    "Δ = gs[W]\n",
    "\n",
    "# Update the parameter and reset the gradient\n",
    "update!(W, -0.1Δ)\n",
    "\n",
    "loss(x, y) # ~ 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "There are a large number of packages intended to be used for optimization in Julia\n",
    "\n",
    "Part of the reason for the diversity of options is that Julia makes it possible to efficiently implement a large number of variations on optimization routines\n",
    "\n",
    "The other reason is that different types of optimization problems require different algorithms\n",
    "\n",
    "TODO: need to describe different types of optimization problems and suggested packages at some point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optim.jl\n",
    "\n",
    "A good pure-Julia solution for the (unconstrained or box-bounded) optimization of univariate and multivariate function is [Optim](https://github.com/JuliaNLSolvers/Optim.jl) package\n",
    "\n",
    "By default, the algorithms in Optim target minimization rather than maximization, so if a function is called “optimize” it will mean minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Functions on Bounded Intervals\n",
    "\n",
    "[Univariate optimization](http://julianlsolvers.github.io/Optim.jl/stable/#user/minimization/#minimizing-a-univariate-function-on-a-bounded-interval) defaults to a robust hybrid optimization routine called Brent’s method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/arnavsood/.julia/compiled/v1.0/Optim/R5uoh.ji for Optim [429524aa-4258-5aef-a3af-852621145aeb]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Brent's Method\n",
       " * Search Interval: [-2.000000, 1.000000]\n",
       " * Minimizer: 0.000000e+00\n",
       " * Minimum: 0.000000e+00\n",
       " * Iterations: 5\n",
       " * Convergence: max(|x - x_upper|, |x - x_lower|) <= 2*(1.5e-08*|x|+2.2e-16): true\n",
       " * Objective Function Calls: 6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim\n",
    "using Optim: converged, maximum, maximizer, minimizer, iterations #some extra functions\n",
    "\n",
    "result = optimize(x-> x^2, -2.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always check if the results converged, and throw errors otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converged(result) || error(\"Failed to converge in $(iterations(result)) iterations\")\n",
    "xmin = result.minimizer\n",
    "result.minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = -x^2\n",
    "result = maximize(f, -2.0, 1.0)\n",
    "converged(result) || error(\"Failed to converge in $(iterations(result)) iterations\")\n",
    "xmin = maximizer(result)\n",
    "fmax = maximum(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are a few inconsistencies with extracting  optimize vs. maximize results, as shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unconstrained Multivariate Optimization\n",
    "\n",
    "There are a variety of [algorithms and options](http://julianlsolvers.github.io/Optim.jl/stable/#user/minimization/#_top) for multivariate optimization\n",
    "\n",
    "From the documentation, the simplest version is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Nelder-Mead\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999634355313174,0.9999315506115275]\n",
       " * Minimum: 3.525527e-09\n",
       " * Iterations: 60\n",
       " * Convergence: true\n",
       "   *  √(Σ(yᵢ-ȳ)²)/n < 1.0e-08: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 117"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv) # i.e. optimize(f, x_iv, NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default algorithm in NelderMead, which is derivative-free and hence requires many function evaluations\n",
    "\n",
    "To change the algorithm type to [L-BFGS](http://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum = 5.378388330692143e-17 with argmin = [1.0, 1.0] in 24 iterations\n"
     ]
    }
   ],
   "source": [
    "results = optimize(f, x_iv, LBFGS())\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in $(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this has fewer iterations\n",
    "\n",
    "As no derivative was given, it used [finite-differences](https://github.com/JuliaDiffEq/DiffEqDiffTools.jl) to approximate the gradient of f(x)\n",
    "\n",
    "However, since most of the algorithms require derivatives, you will often want to use auto differentiation or pass analytical gradients if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum = 5.191703158437428e-27 with argmin = [1.0, 1.0] in 24 iterations\n"
     ]
    }
   ],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv, LBFGS(), autodiff=:forward) # i.e. use ForwardDiff.jl\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in $(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not need to use ForwardDiff.jl directly, as long as our f(x) function was written to be generic (see the [tips and trick](generic_tips_tricks) )\n",
    "\n",
    "Alternatively, with an analytical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum = 4.474575116368833e-22 with argmin = [1.0, 1.0] in 21 iterations\n"
     ]
    }
   ],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "function g!(G, x)\n",
    "    G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "    G[2] = 200.0 * (x[2] - x[1]^2)\n",
    "end\n",
    "\n",
    "results = optimize(f, g!, x0, LBFGS()) # or ConjugateGradient()\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in $(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For derivative-free methods, you can change the algorithm–and have no need to provide a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Simulated Annealing\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.898375086557402,0.8001540211169486]\n",
       " * Minimum: 1.512149e-02\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 0.0e+00: false \n",
       "     |x - x'| = NaN \n",
       "   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n",
       "     |f(x) - f(x')| = NaN |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = NaN \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 1001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv, SimulatedAnnealing()) # or ParticleSwarm() or NelderMead()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you will note that this did not converge, as stochastic methods typically require many more iterations as a tradeoff for their global-convergence properties\n",
    "\n",
    "See the [maximum likelihood](http://julianlsolvers.github.io/Optim.jl/stable/#examples/generated/maxlikenlm/) example and the accompanying [jupyter notebook](https://nbviewer.jupyter.org/github/JuliaNLSolvers/Optim.jl/blob/gh-pages/v0.15.3/examples/generated/maxlikenlm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JuMP.jl\n",
    "\n",
    "The [JuMP.jl](https://github.com/JuliaOpt/JuMP.jl) package is an ambitious implementation of a modelling language for optimization problems in Julia\n",
    "\n",
    "In that sense, it is more like an AMPL (or Pyomo) built on top of the Julia language with macros, and able to use a variety of different commerical and open-source solvers\n",
    "\n",
    "If you have a linear, quadratic, conic, mixed-integer linear, etc. problem then this will likely be the ideal “meta-package” for calling various solvers\n",
    "\n",
    "For nonlinear problems, the modelling language may make things difficult for complicated functions (as it is not designed to be used as a general-purpose non-linear optimizer)\n",
    "\n",
    "See the [quickstart guide](http://www.juliaopt.org/JuMP.jl/0.18/quickstart.html) for more details on all of the options\n",
    "\n",
    "The following is an example of calling a linear objective with a nonlinear constraint (provided by an external function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling JuMP [4076af6c-e467-56ae-b986-b466b2749572]\n",
      "└ @ Base loading.jl:1186\n",
      "┌ Info: Precompiling Ipopt [b6b21f68-93f8-5de0-b562-5493be1d77c9]\n",
      "└ @ Base loading.jl:1186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.12.8, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        2\n",
      "Number of nonzeros in Lagrangian Hessian.............:        3\n",
      "\n",
      "Total number of variables............................:        2\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        1\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 -1.0000000e+00 0.00e+00 2.07e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1 -1.4100714e+00 0.00e+00 5.48e-02  -1.7 3.94e-01    -  1.00e+00 7.36e-01f  1\n",
      "   2 -1.4113851e+00 0.00e+00 2.83e-08  -2.5 9.29e-04    -  1.00e+00 1.00e+00f  1\n",
      "   3 -1.4140632e+00 0.00e+00 1.50e-09  -3.8 1.89e-03    -  1.00e+00 1.00e+00f  1\n",
      "   4 -1.4142117e+00 0.00e+00 1.84e-11  -5.7 1.05e-04    -  1.00e+00 1.00e+00f  1\n",
      "   5 -1.4142136e+00 0.00e+00 8.23e-09  -8.6 1.30e-06    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 5\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -1.4142135740093271e+00   -1.4142135740093271e+00\n",
      "Dual infeasibility......:   8.2280586788385790e-09    8.2280586788385790e-09\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.5059035815063646e-09    2.5059035815063646e-09\n",
      "Overall NLP error.......:   8.2280586788385790e-09    8.2280586788385790e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 6\n",
      "Number of objective gradient evaluations             = 6\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 6\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 6\n",
      "Number of Lagrangian Hessian evaluations             = 5\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      0.113\n",
      "Total CPU secs in NLP function evaluations           =      0.028\n",
      "\n",
      "EXIT: Optimal Solution Found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       ":Optimal"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JuMP, Ipopt\n",
    "# solve\n",
    "# max( x[1] + x[2] )\n",
    "# st sqrt(x[1]^2 + x[2]^2) <= 1\n",
    "\n",
    "function squareroot(x) # pretending we don't know sqrt()\n",
    "    z = x # Initial starting point for Newton’s method\n",
    "    while abs(z*z - x) > 1e-13\n",
    "        z = z - (z*z-x)/(2z)\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "m = Model(solver = IpoptSolver())\n",
    "JuMP.register(m,:squareroot, 1, squareroot, autodiff=true) # need to register user defined functions for AD\n",
    "\n",
    "@variable(m, x[1:2], start=0.5) # start is the initial condition\n",
    "@objective(m, Max, sum(x))\n",
    "@NLconstraint(m, squareroot(x[1]^2+x[2]^2) <= 1)\n",
    "solve(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is an example of a quadratic objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 0.9999999999999899 y = 0.9999999999999792\n",
      "x = 2.7011471240982194 y = 7.2988528759017814\n"
     ]
    }
   ],
   "source": [
    "# solve\n",
    "# min (1-x)^2 + 100(y-x^2)^2)\n",
    "# st x + y >= 10\n",
    "\n",
    "using JuMP,Ipopt\n",
    "m = Model(solver = IpoptSolver(print_level=0)) # settings for the solver, e.g. suppress output\n",
    "@variable(m, x, start = 0.0)\n",
    "@variable(m, y, start = 0.0)\n",
    "\n",
    "@NLobjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))\n",
    "\n",
    "# adding a (linear) constraint\n",
    "@constraint(m, x + y == 10)\n",
    "solve(m)\n",
    "println(\"x = \", getvalue(x), \" y = \", getvalue(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlackBoxOptim.jl\n",
    "\n",
    "Another package for doing global optimization without derivatives is [BlackBoxOptim.jl](https://github.com/robertfeldt/BlackBoxOptim.jl)\n",
    "\n",
    "To see an example from the documentation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimization with optimizer DiffEvoOpt{FitPopulation{Float64},RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},RandomBound{RangePerDimSearchSpace}}\n",
      "0.00 secs, 0 evals, 0 steps\n",
      "\n",
      "Optimization stopped after 10001 steps and 0.07263016700744629 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using BlackBoxOptim.update! in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termination reason: Max number of steps (10000) reached\n",
      "Steps per second = 137697.6043435872\n",
      "Function evals per second = 139638.94643062307\n",
      "Improvements/step = 0.2135\n",
      "Total function evaluations = 10142\n",
      "\n",
      "\n",
      "Best candidate found: [1.0, 1.0]\n",
      "\n",
      "Fitness: 0.000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using BlackBoxOptim\n",
    "\n",
    "function rosenbrock2d(x)\n",
    "return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "end\n",
    "\n",
    "results = bboptimize(rosenbrock2d; SearchRange = (-5.0, 5.0), NumDimensions = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example for [parallel execution](https://github.com/robertfeldt/BlackBoxOptim.jl/blob/master/examples/rosenbrock_parallel.jl) of the objective is provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of Equations and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roots.jl\n",
    "\n",
    "A root of a real function $ f $ on $ [a,b] $ is an $ x \\in [a, b] $ such that $ f(x)=0 $\n",
    "\n",
    "For example, if we plot the function\n",
    "\n",
    "\n",
    "<a id='equation-root-f'></a>\n",
    "<table width=100%><tr style='background-color: #FFFFFF !important;'>\n",
    "<td width=10%></td>\n",
    "<td width=80%>\n",
    "$$\n",
    "f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1\n",
    "$$\n",
    "</td><td width=10% style='text-align:center !important;'>\n",
    "(1)\n",
    "</td></tr></table>\n",
    "\n",
    "with $ x \\in [0,1] $ we get\n",
    "\n",
    "\n",
    "<a id='root-fig'></a>\n",
    "The unique root is approximately 0.408\n",
    "\n",
    "The [Roots](https://github.com/JuliaLang/Roots.jl) package offers the fzero() to find roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40829350427936706"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Roots\n",
    "f(x) = sin(4 * (x - 1/4)) + x + x^20 - 1\n",
    "fzero(f, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLsolve.jl\n",
    "\n",
    "The package [NLsolve.jl](https://github.com/JuliaNLSolvers/NLsolve.jl/) provides functions to solve for multivariate systems of equations and fixed-points\n",
    "\n",
    "From the documentation, to solve for a system of equations without providing a jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/arnavsood/.julia/compiled/v1.0/NLsolve/KFCNP.ji for NLsolve [2774e3e8-f4cf-5e23-947b-6d7e65073b56]\n",
      "└ @ Base loading.jl:1184\n",
      "WARNING: using NLsolve.converged in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results of Nonlinear Solver Algorithm\n",
       " * Algorithm: Trust-region with dogleg and autoscaling\n",
       " * Starting Point: [0.1, 1.2]\n",
       " * Zero: [-7.77555e-17, 1.0]\n",
       " * Inf-norm of residuals: 0.000000\n",
       " * Iterations: 4\n",
       " * Convergence: true\n",
       "   * |x - x'| < 0.0e+00: false\n",
       "   * |f(x)| < 1.0e-08: true\n",
       " * Function Calls (f): 5\n",
       " * Jacobian Calls (df/dx): 5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using NLsolve\n",
    "\n",
    "f(x) = [(x[1]+3)*(x[2]^3-7)+18\n",
    "        sin(x[2]*exp(x[1])-1)] # returns an array\n",
    "\n",
    "results = nlsolve(f, [ 0.1; 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, the algorithm used finite-differences to calculate the jacobian\n",
    "\n",
    "Alternatively, if f(x) is written generically, you can use auto-differentiation with a single setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged=true at root=[-3.7818e-16, 1.0] in 4 iterations and 5 function calls\n"
     ]
    }
   ],
   "source": [
    "results = nlsolve(f, [ 0.1; 1.2], autodiff=:forward)\n",
    "\n",
    "println(\"converged=$(NLsolve.converged(results)) at root=$(results.zero) in $(results.iterations) iterations and $(results.f_calls) function calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing a function with operates in-place (i.e. modifying an argument) may help performance for large systems of equations (and hurt it for small ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged=true at root=[-3.7818e-16, 1.0] in 4 iterations and 5 function calls\n"
     ]
    }
   ],
   "source": [
    "function f!(F, x) # modifies the first argument\n",
    "    F[1] = (x[1]+3)*(x[2]^3-7)+18\n",
    "    F[2] = sin(x[2]*exp(x[1])-1)\n",
    "end\n",
    "\n",
    "results = nlsolve(f!, [ 0.1; 1.2], autodiff=:forward)\n",
    "\n",
    "println(\"converged=$(NLsolve.converged(results)) at root=$(results.zero) in $(results.iterations) iterations and $(results.f_calls) function calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeastSquaresOptim.jl\n",
    "\n",
    "Many optimization problems can be solved using linear or nonlinear least squares\n",
    "\n",
    "Let $ x \\in R^N $ and $ F(x) : R^N \\to R^M $ with $ M \\geq N $, then the nonlinear least squares problem is\n",
    "\n",
    "$$\n",
    "\\min_x F(x)^T F(x)\n",
    "$$\n",
    "\n",
    "While $ F(x)^T F(x) \\to R $, and hence this problem could technically use any nonlinear optimizer, it is useful to exploit the structure of the problem\n",
    "\n",
    "In particular, the Jacobian of $ F(x) $, can be used to approximate the Hessian of the objective\n",
    "\n",
    "As with most nonlinear optimization problems, the benefits will typically become evident only when analytical or automatic differentiation is possible\n",
    "\n",
    "If $ M = N $ and we know a root $ F(x^*) = 0 $ to the system of equations exists, then NLS is the defacto method for solving large **systems of equations**\n",
    "\n",
    "An implementation of NLS is given in [LeastSquaresOptim.jl](https://github.com/matthieugomez/LeastSquaresOptim.jl)\n",
    "\n",
    "From the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/arnavsood/.julia/compiled/v1.0/LeastSquaresOptim/hdNWC.ji for LeastSquaresOptim [0fc2ff8b-aaa3-5acd-a817-1944a5e08891]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Dogleg\n",
       " * Minimizer: [1.0,1.0]\n",
       " * Sum of squares at Minimum: 0.000000\n",
       " * Iterations: 51\n",
       " * Convergence: true\n",
       " * |x - x'| < 1.0e-08: false\n",
       " * |f(x) - f(x')| / |f(x)| < 1.0e-08: true\n",
       " * |g(x)| < 1.0e-08: false\n",
       " * Function Calls: 52\n",
       " * Gradient Calls: 36\n",
       " * Multiplication Calls: 159\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LeastSquaresOptim\n",
    "function rosenbrock(x)\n",
    "    [1 - x[1], 100 * (x[2]-x[1]^2)]\n",
    "end\n",
    "LeastSquaresOptim.optimize(rosenbrock, zeros(2), Dogleg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Because there is a name clash between Optim.jl and this package, to use both we need to qualify the use of the optimize function (i.e. LeastSquaresOptim.optimize)\n",
    "\n",
    "Here, by default it will use AD with ForwardDiff.jl to calculate the Jacobian, but you could also provide your own calculation of the Jacobian (analytical or using finite differences) and/or calculate the function in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Dogleg\n",
       " * Minimizer: [1.0,1.0]\n",
       " * Sum of squares at Minimum: 0.000000\n",
       " * Iterations: 51\n",
       " * Convergence: true\n",
       " * |x - x'| < 1.0e-08: false\n",
       " * |f(x) - f(x')| / |f(x)| < 1.0e-08: true\n",
       " * |g(x)| < 1.0e-08: false\n",
       " * Function Calls: 52\n",
       " * Gradient Calls: 36\n",
       " * Multiplication Calls: 159\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rosenbrock_f!(out, x)\n",
    "    out[1] = 1 - x[1]\n",
    "    out[2] = 100 * (x[2]-x[1]^2)\n",
    "end\n",
    "LeastSquaresOptim.optimize!(LeastSquaresProblem(x = zeros(2), f! = rosenbrock_f!, output_length = 2))\n",
    "\n",
    "# if you want to use gradient\n",
    "function rosenbrock_g!(J, x)\n",
    "    J[1, 1] = -1\n",
    "    J[1, 2] = 0\n",
    "    J[2, 1] = -200 * x[1]\n",
    "    J[2, 2] = 100\n",
    "end\n",
    "LeastSquaresOptim.optimize!(LeastSquaresProblem(x = zeros(2), f! = rosenbrock_f!, g! = rosenbrock_g!, output_length = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Watch [this video](https://www.youtube.com/watch?v=vAp6nUMrKYg&feature=youtu.be) from one of Julia’s creators on automatic differentiation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0",
   "language": "julia",
   "name": "julia-1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
